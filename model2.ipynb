{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90044eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\doand\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "80f648f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'check_update_conditions' from 'helper' (D:\\OneDrive\\Máy tính\\snake_game_AI\\helper.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhelper\u001b[39;00m \n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_update_conditions\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'check_update_conditions' from 'helper' (D:\\OneDrive\\Máy tính\\snake_game_AI\\helper.py)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.losses import MSE\n",
    "from keras.optimizers import Adam\n",
    "from game import SnakeGameAI, Direction, Point\n",
    "from agent import Agent\n",
    "import helper \n",
    "from helper import check_update_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82bbee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(helper.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "706d1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6c10a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 11\n",
    "num_actions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83bdaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75b26546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(shape=state_size),  \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(num_actions, activation = 'linear'),\n",
    "    ### END CODE HERE ### \n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(shape=state_size),  \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(num_actions, activation = 'linear'),\n",
    "    ### END CODE HERE ###\n",
    "    ])\n",
    "\n",
    "### START CODE HERE ### \n",
    "optimizer = Adam(learning_rate = ALPHA)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6b167ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    ### START CODE HERE ### \n",
    "    y_targets = rewards + gamma * (max_qsa * (1-done_vals))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    ### START CODE HERE ### \n",
    "    loss = MSE(y_targets, q_values)\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5626cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    helper.update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e446b2c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'check_update_conditions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m next_state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_state(game)\n\u001b[0;32m     39\u001b[0m memory_buffer\u001b[38;5;241m.\u001b[39mappend(experience(state, action, reward, next_state, done))\n\u001b[1;32m---> 41\u001b[0m update \u001b[38;5;241m=\u001b[39m helper\u001b[38;5;241m.\u001b[39mcheck_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update:\n\u001b[0;32m     45\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_experiences(memory_buffer)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helper' has no attribute 'check_update_conditions'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100 # number of total point to use for averaging\n",
    "epsilon = 1.0 # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "agent = Agent()\n",
    "game = SnakeGameAI()\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # reset the environment to the initial state and get the inital state\n",
    "    game.reset()\n",
    "    state = agent.get_state(game)\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        \n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        q_values = q_network(state_qn)\n",
    "        \n",
    "        # get move\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(action)\n",
    "        next_state = agent.get_state(game)\n",
    "        \n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        update = helper.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            \n",
    "            experiences = utils.get_experiences(memory_buffer)\n",
    "            \n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('lunar_lander_model.h5')\n",
    "        break\n",
    "\n",
    "tot_time = time.time() - start\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1473936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function helper.get_new_eps(epsilon)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.get_new_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155312fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
