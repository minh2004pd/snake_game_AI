{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd5c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\doand\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e394ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.losses import MSE\n",
    "from keras.optimizers import Adam\n",
    "from game import SnakeGameAI, Direction, Point\n",
    "from agent import Agent\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e0bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(helper.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6bba533",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b6d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 11\n",
    "num_actions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9da596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c20c11e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\doand\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(shape=state_size),  \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(num_actions, activation = 'linear'),\n",
    "    ### END CODE HERE ### \n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(shape=state_size),  \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(num_actions, activation = 'linear'),\n",
    "    ### END CODE HERE ###\n",
    "    ])\n",
    "\n",
    "### START CODE HERE ### \n",
    "optimizer = Adam(learning_rate = ALPHA)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a904ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    ### START CODE HERE ### \n",
    "    y_targets = rewards + gamma * (max_qsa * (1-done_vals))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    ### START CODE HERE ### \n",
    "    loss = MSE(y_targets, q_values)\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c74b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    helper.update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a764a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 Score 0 Record: 0\n",
      "Game 2 Score 0 Record: 0\n",
      "Game 3 Score 0 Record: 0\n",
      "Game 4 Score 0 Record: 0\n",
      "Game 5 Score 0 Record: 0\n",
      "Game 6 Score 0 Record: 0\n",
      "Game 7 Score 0 Record: 0\n",
      "Game 8 Score 0 Record: 0\n",
      "Game 9 Score 0 Record: 0\n",
      "Game 10 Score 0 Record: 0\n",
      "Game 11 Score 0 Record: 0\n",
      "Game 12 Score 0 Record: 0\n",
      "Game 13 Score 0 Record: 0\n",
      "Game 14 Score 0 Record: 0\n",
      "Game 15 Score 0 Record: 0\n",
      "Game 16 Score 0 Record: 0\n",
      "Game 17 Score 0 Record: 0\n",
      "Game 18 Score 0 Record: 0\n",
      "Game 19 Score 0 Record: 0\n",
      "Game 20 Score 0 Record: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# perform move and get new state\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m reward, done, score \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mplay_step(action)\n\u001b[0;32m     39\u001b[0m next_state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_state(game)\n\u001b[0;32m     41\u001b[0m memory_buffer\u001b[38;5;241m.\u001b[39mappend(experience(state, action, reward, next_state, done))\n",
      "File \u001b[1;32mD:\\OneDrive\\Máy tính\\snake_game_AI\\game.py:96\u001b[0m, in \u001b[0;36mSnakeGameAI.play_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# 5. update ui and clock\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ui()\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(SPEED)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# 6. return game over and score\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward, game_over, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1500\n",
    "\n",
    "total_point_history = []\n",
    "record = 0\n",
    "\n",
    "num_p_av = 100 # number of total point to use for averaging\n",
    "epsilon = 1.0 # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "agent = Agent()\n",
    "game = SnakeGameAI()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # reset the environment to the initial state and get the inital state\n",
    "    state = agent.get_state(game)\n",
    "    total_points = 0\n",
    "    t = 0\n",
    "    \n",
    "    while True:\n",
    "        t += 1\n",
    "        \n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        q_values = q_network(state_qn)\n",
    "        \n",
    "        # get move\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(action)\n",
    "        next_state = agent.get_state(game)\n",
    "        \n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        update = helper.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            \n",
    "            experiences = helper.get_experiences(memory_buffer)\n",
    "            \n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            game.reset()\n",
    "            agent.n_games += 1\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "                \n",
    "            print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
    "\n",
    "#             plot_scores.append(score)\n",
    "#             total_score += score\n",
    "#             mean_score = total_score / agent.n_games\n",
    "#             plot_mean_scores.append(mean_score)\n",
    "#             plot(plot_scores, plot_mean_scores)\n",
    "    \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('lunar_lander_model.h5')\n",
    "        break\n",
    "\n",
    "tot_time = time.time() - start\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc59688b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m state_qn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "state_qn = np.expand_dims(state, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b850b0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_qn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b798551",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = q_network(state_qn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "980be8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.22272423,  0.30968338,  0.02527073]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2ea36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "game = SnakeGameAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bed9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112c068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
